import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import os
import sys
import time
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# ä¿®å¤ï¼šå¯¼å…¥æ­£ç¡®çš„ç±»å
from model import VSA_ATD_VisionTransformer
from configs.config import VSA_ATD_Config

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨ - æ£€æµ‹æŸå¤±å˜åŒ–ã€é›¶æ¢¯åº¦å æ¯”å’Œè®­ç»ƒé”™è¯¯"""
    
    def __init__(self, model, patience=5):
        self.model = model
        self.patience = patience
        
        # æŸå¤±å†å²è®°å½•
        self.loss_history = []
        self.loss_change_history = []
        
        # æ¢¯åº¦ç»Ÿè®¡
        self.zero_grad_ratios = []
        self.grad_norm_history = []
        
        # è®­ç»ƒçŠ¶æ€ç›‘æ§
        self.stagnant_epochs = 0
        self.error_flags = []
        
        # å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
        self.min_loss_change = 1e-6  # æœ€å°æŸå¤±å˜åŒ–é˜ˆå€¼
        self.max_zero_grad_ratio = 0.8  # æœ€å¤§é›¶æ¢¯åº¦æ¯”ä¾‹
        self.max_grad_norm = 10.0  # æœ€å¤§æ¢¯åº¦èŒƒæ•°
        self.min_grad_norm = 1e-8  # æœ€å°æ¢¯åº¦èŒƒæ•°
        
    def check_loss_change(self, current_loss):
        """æ£€æŸ¥æŸå¤±å˜åŒ–"""
        self.loss_history.append(current_loss)
        
        if len(self.loss_history) < 2:
            return True, "é¦–æ¬¡è®°å½•æŸå¤±"
            
        # è®¡ç®—æŸå¤±å˜åŒ–
        loss_change = abs(self.loss_history[-1] - self.loss_history[-2])
        self.loss_change_history.append(loss_change)
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦åœæ»
        if loss_change < self.min_loss_change:
            self.stagnant_epochs += 1
            if self.stagnant_epochs >= self.patience:
                return False, f"æŸå¤±è¿ç»­{self.stagnant_epochs}è½®å˜åŒ–è¿‡å° (<{self.min_loss_change})"
        else:
            self.stagnant_epochs = 0
            
        return True, f"æŸå¤±å˜åŒ–: {loss_change:.6f}"
    
    def check_gradient_health(self):
        """æ£€æŸ¥æ¢¯åº¦å¥åº·çŠ¶å†µ"""
        total_params = 0
        zero_grad_params = 0
        total_grad_norm = 0.0
        nan_grad_params = 0
        inf_grad_params = 0
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                total_params += 1
                
                if param.grad is None:
                    zero_grad_params += 1
                else:
                    grad_norm = param.grad.norm().item()
                    
                    # æ£€æŸ¥NaNå’ŒInf
                    if torch.isnan(param.grad).any():
                        nan_grad_params += 1
                    elif torch.isinf(param.grad).any():
                        inf_grad_params += 1
                    elif grad_norm < self.min_grad_norm:
                        zero_grad_params += 1
                    else:
                        total_grad_norm += grad_norm
        
        # è®¡ç®—é›¶æ¢¯åº¦æ¯”ä¾‹
        zero_grad_ratio = zero_grad_params / max(total_params, 1)
        avg_grad_norm = total_grad_norm / max(total_params - zero_grad_params, 1)
        
        self.zero_grad_ratios.append(zero_grad_ratio)
        self.grad_norm_history.append(avg_grad_norm)
        
        # æ£€æŸ¥å¼‚å¸¸æƒ…å†µ
        errors = []
        
        if zero_grad_ratio > self.max_zero_grad_ratio:
            errors.append(f"é›¶æ¢¯åº¦æ¯”ä¾‹è¿‡é«˜: {zero_grad_ratio:.2%}")
            
        if avg_grad_norm > self.max_grad_norm:
            errors.append(f"æ¢¯åº¦èŒƒæ•°è¿‡å¤§: {avg_grad_norm:.2e}")
            
        if avg_grad_norm < self.min_grad_norm:
            errors.append(f"æ¢¯åº¦èŒƒæ•°è¿‡å°: {avg_grad_norm:.2e}")
            
        if nan_grad_params > 0:
            errors.append(f"å‘ç°{nan_grad_params}ä¸ªNaNæ¢¯åº¦å‚æ•°")
            
        if inf_grad_params > 0:
            errors.append(f"å‘ç°{inf_grad_params}ä¸ªInfæ¢¯åº¦å‚æ•°")
        
        return {
            'zero_grad_ratio': zero_grad_ratio,
            'avg_grad_norm': avg_grad_norm,
            'total_params': total_params,
            'zero_grad_params': zero_grad_params,
            'nan_grad_params': nan_grad_params,
            'inf_grad_params': inf_grad_params,
            'errors': errors
        }
    
    def detect_training_errors(self, epoch, batch_idx, loss, accuracy=None):
        """æ£€æµ‹è®­ç»ƒé”™è¯¯"""
        errors = []
        warnings = []
        
        # 1. æ£€æŸ¥æŸå¤±å€¼å¼‚å¸¸
        if torch.isnan(torch.tensor(loss)):
            errors.append("æŸå¤±å€¼ä¸ºNaN")
        elif torch.isinf(torch.tensor(loss)):
            errors.append("æŸå¤±å€¼ä¸ºInf")
        elif loss < 0:
            errors.append(f"æŸå¤±å€¼ä¸ºè´Ÿæ•°: {loss:.6f}")
        elif loss > 100:
            warnings.append(f"æŸå¤±å€¼è¿‡å¤§: {loss:.6f}")
            
        # 2. æ£€æŸ¥å‡†ç¡®ç‡å¼‚å¸¸
        if accuracy is not None:
            if accuracy < 0 or accuracy > 100:
                errors.append(f"å‡†ç¡®ç‡å¼‚å¸¸: {accuracy:.2f}%")
            elif accuracy == 0 and epoch > 2:
                warnings.append("å‡†ç¡®ç‡ä¸º0ï¼Œå¯èƒ½å­˜åœ¨æ ‡ç­¾é—®é¢˜")
                
        # 3. æ£€æŸ¥æŸå¤±å˜åŒ–
        loss_ok, loss_msg = self.check_loss_change(loss)
        if not loss_ok:
            warnings.append(loss_msg)
            
        # 4. æ£€æŸ¥æ¢¯åº¦å¥åº·çŠ¶å†µ
        grad_stats = self.check_gradient_health()
        errors.extend(grad_stats['errors'])
        
        # 5. æ£€æŸ¥è®­ç»ƒè¶‹åŠ¿
        if len(self.loss_history) >= 5:
            recent_losses = self.loss_history[-5:]
            if all(l1 <= l2 for l1, l2 in zip(recent_losses[:-1], recent_losses[1:])):
                warnings.append("æŸå¤±è¿ç»­5è½®ä¸Šå‡ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
                
        return {
            'errors': errors,
            'warnings': warnings,
            'grad_stats': grad_stats,
            'loss_change': loss_msg
        }
    
    def print_status(self, epoch, batch_idx, loss, accuracy=None):
        """æ‰“å°è®­ç»ƒçŠ¶æ€"""
        status = self.detect_training_errors(epoch, batch_idx, loss, accuracy)
        
        print(f"\nğŸ“Š è®­ç»ƒç›‘æ§ - Epoch {epoch}, Batch {batch_idx}")
        print("-" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ’¡ å½“å‰çŠ¶æ€:")
        print(f"  æŸå¤±: {loss:.6f}")
        if accuracy is not None:
            print(f"  å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"  {status['loss_change']}")
        
        # æ¢¯åº¦ç»Ÿè®¡
        grad_stats = status['grad_stats']
        print(f"\nğŸ” æ¢¯åº¦ç»Ÿè®¡:")
        print(f"  é›¶æ¢¯åº¦æ¯”ä¾‹: {grad_stats['zero_grad_ratio']:.2%} ({grad_stats['zero_grad_params']}/{grad_stats['total_params']})")
        print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {grad_stats['avg_grad_norm']:.2e}")
        
        if grad_stats['nan_grad_params'] > 0:
            print(f"  NaNæ¢¯åº¦å‚æ•°: {grad_stats['nan_grad_params']}")
        if grad_stats['inf_grad_params'] > 0:
            print(f"  Infæ¢¯åº¦å‚æ•°: {grad_stats['inf_grad_params']}")
        
        # é”™è¯¯å’Œè­¦å‘Š
        if status['errors']:
            print(f"\nğŸš¨ é”™è¯¯:")
            for error in status['errors']:
                print(f"  âŒ {error}")
                
        if status['warnings']:
            print(f"\nâš ï¸  è­¦å‘Š:")
            for warning in status['warnings']:
                print(f"  ğŸŸ¡ {warning}")
                
        if not status['errors'] and not status['warnings']:
            print(f"\nâœ… è®­ç»ƒçŠ¶æ€æ­£å¸¸")
            
        return len(status['errors']) == 0
    
    def get_summary(self):
        """è·å–è®­ç»ƒæ€»ç»“"""
        if not self.loss_history:
            return "æ— è®­ç»ƒæ•°æ®"
            
        summary = {
            'total_epochs': len(self.loss_history),
            'final_loss': self.loss_history[-1],
            'loss_reduction': self.loss_history[0] - self.loss_history[-1] if len(self.loss_history) > 1 else 0,
            'avg_zero_grad_ratio': np.mean(self.zero_grad_ratios) if self.zero_grad_ratios else 0,
            'avg_grad_norm': np.mean(self.grad_norm_history) if self.grad_norm_history else 0,
            'stagnant_periods': self.stagnant_epochs
        }
        
        return summary

# ... existing code ...

def train_model():
    """è®­ç»ƒæ¨¡å‹ä¸»å‡½æ•°"""
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®åŠ è½½
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])
    
    transform_test = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])
    
    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
    val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)
    
    # æ¨¡å‹é…ç½®å’Œåˆå§‹åŒ–
    config = VSA_ATD_Config()
    model = VSA_ATD_VisionTransformer(config).to(device)
    
    # åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨
    monitor = TrainingMonitor(model, patience=3)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 20
    best_acc = 0.0
    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ - åŒ…å«æ™ºèƒ½è®­ç»ƒç›‘æ§")
    print("=" * 100)
    
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        
        # ==================== è®­ç»ƒé˜¶æ®µ ====================
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{num_epochs} - è®­ç»ƒé˜¶æ®µ")
        print("-" * 60)
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # æ¸…é›¶æ¢¯åº¦
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            output = model(data)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # è®¡ç®—å½“å‰å‡†ç¡®ç‡
            _, predicted = output.max(1)
            batch_correct = predicted.eq(target).sum().item()
            batch_acc = 100. * batch_correct / target.size(0)
            
            # è®­ç»ƒç›‘æ§ï¼ˆæ¯50ä¸ªbatchæ£€æŸ¥ä¸€æ¬¡ï¼‰
            if batch_idx % 50 == 0:
                training_ok = monitor.print_status(epoch+1, batch_idx, loss.item(), batch_acc)
                
                # å¦‚æœæ£€æµ‹åˆ°ä¸¥é‡é”™è¯¯ï¼Œåœæ­¢è®­ç»ƒ
                if not training_ok:
                    print(f"\nğŸ›‘ æ£€æµ‹åˆ°ä¸¥é‡è®­ç»ƒé”™è¯¯ï¼Œåœæ­¢è®­ç»ƒ")
                    return None
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            # å‚æ•°æ›´æ–°
            optimizer.step()
            
            # ç»Ÿè®¡
            train_loss += loss.item()
            train_total += target.size(0)
            train_correct += batch_correct
            
            # è¿›åº¦æ˜¾ç¤º
            if batch_idx % 100 == 0:
                current_acc = 100. * train_correct / train_total
                print(f'  Batch {batch_idx:3d}: Loss={loss.item():.6f}, Acc={current_acc:.2f}%')
        
        # ==================== éªŒè¯é˜¶æ®µ ====================
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs} - éªŒè¯é˜¶æ®µ")
        print("-" * 60)
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        # è®°å½•å†å²
        train_losses.append(avg_train_loss)
        train_accs.append(train_acc)
        val_losses.append(avg_val_loss)
        val_accs.append(val_acc)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è¾“å‡ºepochç»“æœ
        epoch_time = time.time() - epoch_start_time
        print(f"\nğŸ“‹ Epoch {epoch+1}/{num_epochs} ç»“æœ:")
        print(f"  è®­ç»ƒ - æŸå¤±: {avg_train_loss:.6f}, å‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"  éªŒè¯ - æŸå¤±: {avg_val_loss:.6f}, å‡†ç¡®ç‡: {val_acc:.2f}%")
        print(f"  å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.8f}")
        print(f"  è€—æ—¶: {epoch_time:.2f}ç§’")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_acc': best_acc,
                'config': config.__dict__,
                'train_history': {
                    'train_losses': train_losses,
                    'train_accs': train_accs,
                    'val_losses': val_losses,
                    'val_accs': val_accs
                }
            }, 'best_vsa_atd_vit.pth')
            print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%)")
        
        print("=" * 100)
    
    # è®­ç»ƒå®Œæˆæ€»ç»“
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accs[-1]:.2f}%")
    print(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}")
    
    # æ‰“å°è®­ç»ƒç›‘æ§æ€»ç»“
    summary = monitor.get_summary()
    print(f"\nğŸ“ˆ è®­ç»ƒç›‘æ§æ€»ç»“:")
    print(f"  æ€»è®­ç»ƒè½®æ•°: {summary['total_epochs']}")
    print(f"  æŸå¤±ä¸‹é™: {summary['loss_reduction']:.6f}")
    print(f"  å¹³å‡é›¶æ¢¯åº¦æ¯”ä¾‹: {summary['avg_zero_grad_ratio']:.2%}")
    print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {summary['avg_grad_norm']:.2e}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.plot(val_losses, label='éªŒè¯æŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('æŸå¤±')
    plt.legend()
    plt.title('æŸå¤±æ›²çº¿')
    
    plt.subplot(1, 3, 2)
    plt.plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡')
    plt.plot(val_accs, label='éªŒè¯å‡†ç¡®ç‡')
    plt.xlabel('Epoch')
    plt.ylabel('å‡†ç¡®ç‡ (%)')
    plt.legend()
    plt.title('å‡†ç¡®ç‡æ›²çº¿')
    
    plt.subplot(1, 3, 3)
    if monitor.zero_grad_ratios:
        plt.plot(monitor.zero_grad_ratios, label='é›¶æ¢¯åº¦æ¯”ä¾‹')
        plt.xlabel('æ£€æŸ¥ç‚¹')
        plt.ylabel('æ¯”ä¾‹')
        plt.legend()
        plt.title('é›¶æ¢¯åº¦æ¯”ä¾‹å˜åŒ–')
    
    plt.tight_layout()
    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')
    print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º training_curves.png")
    
    return {
        'best_acc': best_acc,
        'train_history': {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs
        },
        'monitor_summary': summary
    }

if __name__ == '__main__':
    try:
        results = train_model()
        if results:
            print(f"\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
            print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {results['best_acc']:.2f}%")
        else:
            print(f"\nâŒ è®­ç»ƒå› é”™è¯¯è€Œç»ˆæ­¢")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()